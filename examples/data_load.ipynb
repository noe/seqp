{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading : batching, bucketing\n",
    "\n",
    "\n",
    "In deep learning, it is common to process the training data in \"mini-batches\", at each optimization step we only use a small random sample of the data.\n",
    "\n",
    "For image processing tasks, all images are of the same size; however, for some sequence processing tasks like text processing, the length of each sequence is arbitrary. This means that when we group together some sentences in a minibatch, we need to add \"padding\" tokens to make all sentences as long as the longest sentence in the minibatch. This is a waste of space in the minibatch and a way to paliate this problem is to try to make batches with sentences of similar length, so that we minimize the amount of padding introduced. This way of preparing batches based on the sequence length is known as \"bucketing\".\n",
    "\n",
    "`seqp` offers you bucketed sequence batch loading out of the box. This notebook illustrates such a feature in a very flexible way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q http://pcai056.informatik.uni-leipzig.de/downloads/corpora/eng_news_2016_10K.tar.gz\n",
    "!tar xzf eng_news_2016_10K.tar.gz\n",
    "!cut -f2 eng_news_2016_10K/eng_news_2016_10K-sentences.txt | gshuf > ./corpus.en\n",
    "!rm -rf eng_news_2016_10K eng_news_2016_10K.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from seqp.vocab import Vocabulary, VocabularyCollector\n",
    "\n",
    "file_name = 'corpus.en'\n",
    "\n",
    "collector = VocabularyCollector()\n",
    "\n",
    "with open(file_name) as f:\n",
    "    for line in f:\n",
    "        line = line.strip().lower()\n",
    "        # tokenize words (taken from https://stackoverflow.com/a/8930959/674487)\n",
    "        tokens = re.findall(r\"\\w+|[^\\w\\s]\", line, re.UNICODE)\n",
    "        for token in tokens:\n",
    "            collector.add_symbol(token)\n",
    "\n",
    "vocab = collector.consolidate(max_num_symbols=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from seqp.record import ShardedWriter\n",
    "from seqp.hdf5 import Hdf5RecordWriter\n",
    "\n",
    "\n",
    "with ShardedWriter(Hdf5RecordWriter,\n",
    "                   'corpus.shard_{:02d}.hdf5',\n",
    "                   max_records_per_shard=4000) as writer, open(file_name) as f:\n",
    "\n",
    "    # save vocabulary along with the records\n",
    "    writer.add_metadata({'vocab': vocab.to_json()})\n",
    "\n",
    "    for idx, line in enumerate(f):\n",
    "        line = line.strip().lower()\n",
    "        tokens = re.findall(r\"\\w+|[^\\w\\s]\", line, re.UNICODE)\n",
    "        token_ids = vocab.encode(tokens, add_eos=False, use_unk=True)\n",
    "        writer.write(idx, np.array(token_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 000. num_tokens=480, shape=(20, 24)\n",
      "Batch 001. num_tokens=496, shape=(31, 16)\n",
      "Batch 002. num_tokens=496, shape=(31, 16)\n",
      "Batch 003. num_tokens=492, shape=(41, 12)\n",
      "Batch 004. num_tokens=493, shape=(17, 29)\n",
      "Batch 005. num_tokens=496, shape=(31, 16)\n",
      "Batch 006. num_tokens=500, shape=(25, 20)\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "from seqp.iteration import DataLoader\n",
    "from seqp.hdf5 import Hdf5RecordReader\n",
    "from seqp.vocab import Vocabulary\n",
    "\n",
    "BATCH_SIZE_IN_TOKENS = 500\n",
    "\n",
    "with Hdf5RecordReader(glob('corpus.shard_*.hdf5')) as reader:\n",
    "    vocab = Vocabulary.from_json(reader.metadata('vocab'))\n",
    "    \n",
    "    loader = DataLoader(reader, pad_value=vocab.pad_id, num_buckets=8) \n",
    "    \n",
    "    batch_it = loader.iterator(batch_size=BATCH_SIZE_IN_TOKENS,\n",
    "                               is_size_in_tokens=True)\n",
    "    \n",
    "    for k, batch in enumerate(batch_it):\n",
    "        num_tokens = batch.shape[0] * batch.shape[1]\n",
    "        print(\"Batch {:03d}. num_tokens={}, shape={}\".format(k, num_tokens, batch.shape))\n",
    "        if k > 5:\n",
    "            break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
